name: Intelligent Skill Dependency Analyzer & Optimizer

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'skills/**'
      - '.github/workflows/intelligent-skill-optimizer.yml'
  pull_request:
    paths:
      - 'skills/**'
  schedule:
    # Run optimization analysis daily at 2 AM
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      optimization_target:
        description: 'Optimization target'
        required: true
        type: choice
        options:
          - 'performance'
          - 'token_efficiency'
          - 'security'
          - 'maintainability'
          - 'comprehensive'
        default: 'comprehensive'
      analysis_depth:
        description: 'Analysis depth'
        required: true
        type: choice
        options:
          - 'shallow'
          - 'moderate'
          - 'deep'
          - 'exhaustive'
        default: 'moderate'
      generate_recommendations:
        description: 'Generate optimization recommendations'
        type: boolean
        default: true

env:
  PYTHON_VERSION: '3.11'
  ANALYSIS_TIMEOUT: '3600'  # 1 hour for deep analysis
  ML_MODEL_VERSION: 'v2.1.0'

jobs:
  # Phase 1: Data Collection & Preprocessing
  collect-skill-data:
    name: Collect Skill Intelligence Data
    runs-on: ubuntu-latest
    outputs:
      skill-metrics: ${{ steps.metrics.outputs.data }}
      dependency-graph: ${{ steps.dependencies.outputs.graph }}
      performance-baseline: ${{ steps.baseline.outputs.baseline }}
    steps:
      - name: Checkout code with full history
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python with ML libraries
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ML and analysis dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn networkx matplotlib seaborn plotly
          pip install transformers torch nltk spacy
          pip install pyyaml jsonschema psutil

      - name: Collect skill usage metrics
        id: metrics
        run: |
          echo "ðŸ“Š Collecting skill usage metrics..."
          
          python -c "
          import json
          import os
          from pathlib import Path
          from datetime import datetime, timedelta
          import subprocess
          
          def collect_skill_metrics():
              metrics = {
                  'skills': {},
                  'categories': {},
                  'global_stats': {
                      'total_skills': 0,
                      'total_categories': 0,
                      'analysis_timestamp': datetime.now().isoformat()
                  }
              }
              
              skills_dir = Path('skills')
              
              # Get git history for usage patterns
              try:
                  git_log = subprocess.run(['git', 'log', '--since=30.days', '--pretty=format:%h|%an|%ad|%s', '--date=iso'], 
                                         capture_output=True, text=True, check=True)
                  commit_history = git_log.stdout.split('\n')
              except:
                  commit_history = []
              
              for category_dir in skills_dir.iterdir():
                  if category_dir.is_dir() and not category_dir.name.startswith('.'):
                      category = category_dir.name
                      metrics['categories'][category] = {
                          'skill_count': 0,
                          'total_tokens': 0,
                          'avg_complexity': 0,
                          'skills': []
                      }
                      
                      for skill_dir in category_dir.iterdir():
                          if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                              skill_name = skill_dir.name
                              
                              # Analyze skill file
                              skill_file = skill_dir / 'SKILL.md'
                              if skill_file.exists():
                                  content = skill_file.read_text()
                                  
                                  # Calculate metrics
                                  word_count = len(content.split())
                                  char_count = len(content)
                                  line_count = len(content.split('\n'))
                                  
                                  # Estimate tokens (rough approximation)
                                  estimated_tokens = int(word_count * 1.3)
                                  
                                  # Calculate complexity score
                                  complexity_factors = {
                                      'code_blocks': content.count('```'),
                                      'steps': len([line for line in content.split('\n') if line.strip().startswith(('1.', '2.', '3.'))]),
                                      'conditionals': content.lower().count('if ') + content.lower().count('when '),
                                      'loops': content.lower().count('for ') + content.lower().count('while '),
                                      'external_refs': content.count('http') + content.count('import')
                                  }
                                  
                                  complexity_score = sum(complexity_factors.values()) * 0.2 + (word_count / 100)
                                  
                                  # Get git activity for this skill
                                  skill_activity = []
                                  for commit in commit_history:
                                      if skill_name in commit.lower():
                                          parts = commit.split('|')
                                          if len(parts) >= 4:
                                              skill_activity.append({
                                                  'hash': parts[0],
                                                  'author': parts[1],
                                                  'date': parts[2],
                                                  'message': parts[3]
                                              })
                                  
                                  skill_data = {
                                      'name': skill_name,
                                      'category': category,
                                      'metrics': {
                                          'word_count': word_count,
                                          'char_count': char_count,
                                          'line_count': line_count,
                                          'estimated_tokens': estimated_tokens,
                                          'complexity_score': round(complexity_score, 2),
                                          'complexity_factors': complexity_factors
                                      },
                                      'activity': {
                                          'recent_commits': len(skill_activity),
                                          'last_modified': skill_activity[0]['date'] if skill_activity else 'unknown'
                                      },
                                      'quality_indicators': {
                                          'has_examples': '```' in content,
                                          'has_documentation': (skill_dir / 'README.md').exists(),
                                          'has_template': (skill_dir / 'template.md').exists(),
                                          'purpose_defined': 'purpose:' in content.lower(),
                                          'usage_defined': 'usage:' in content.lower()
                                      }
                                  }
                                  
                                  metrics['skills'][f'{category}/{skill_name}'] = skill_data
                                  metrics['categories'][category]['skills'].append(skill_name)
                                  metrics['categories'][category]['skill_count'] += 1
                                  metrics['categories'][category]['total_tokens'] += estimated_tokens
                                  metrics['global_stats']['total_skills'] += 1
                      
                      # Calculate category averages
                      if metrics['categories'][category]['skill_count'] > 0:
                          avg_tokens = metrics['categories'][category]['total_tokens'] / metrics['categories'][category]['skill_count']
                          avg_complexity = sum(metrics['skills'][f'{category}/{skill}']['metrics']['complexity_score'] 
                                             for skill in metrics['categories'][category]['skills']) / metrics['categories'][category]['skill_count']
                          
                          metrics['categories'][category]['avg_tokens'] = round(avg_tokens, 2)
                          metrics['categories'][category]['avg_complexity'] = round(avg_complexity, 2)
          
          # Save metrics
          with open('skill-metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          print(f'ðŸ“Š Collected metrics for {metrics["global_stats"]["total_skills"]} skills')
          return metrics
          
          if __name__ == '__main__':
              collect_skill_metrics()
          "
          
          echo "data=skill-metrics.json" >> $GITHUB_OUTPUT

      - name: Build dependency graph
        id: dependencies
        run: |
          echo "ðŸ”— Building skill dependency graph..."
          
          python -c "
          import json
          import networkx as nx
          from pathlib import Path
          import re
          
          def build_dependency_graph():
              G = nx.DiGraph()
              
              # Add nodes for all skills
              skills_dir = Path('skills')
              for category_dir in skills_dir.iterdir():
                  if category_dir.is_dir() and not category_dir.name.startswith('.'):
                      category = category_dir.name
                      for skill_dir in category_dir.iterdir():
                          if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                              skill_name = skill_dir.name
                              node_id = f'{category}/{skill_name}'
                              
                              # Add node with attributes
                              G.add_node(node_id, 
                                       category=category,
                                       name=skill_name)
              
              # Add edges based on dependencies
              for node_id in G.nodes():
                  category, skill_name = node_id.split('/')
                  skill_dir = Path(f'skills/{category}/{skill_name}')
                  skill_file = skill_dir / 'SKILL.md'
                  
                  if skill_file.exists():
                      content = skill_file.read_text().lower()
                      
                      # Look for references to other skills
                      for other_node in G.nodes():
                          if other_node != node_id:
                              other_category, other_skill = other_node.split('/')
                              
                              # Check for skill invocation patterns
                              patterns = [
                                  f'/{other_skill}',
                                  f'``{other_skill}``',
                                  f'`{other_skill}`',
                                  other_skill.lower()
                              ]
                              
                              for pattern in patterns:
                                  if pattern in content:
                                      G.add_edge(node_id, other_node)
                                      break
              
              # Analyze graph properties
              analysis = {
                  'graph_stats': {
                      'nodes': G.number_of_nodes(),
                      'edges': G.number_of_edges(),
                      'density': nx.density(G),
                      'is_connected': nx.is_weakly_connected(G),
                      'connected_components': nx.number_weakly_connected_components(G)
                  },
                  'centrality_analysis': {
                      'degree_centrality': nx.degree_centrality(G),
                      'betweenness_centrality': nx.betweenness_centrality(G),
                      'closeness_centrality': nx.closeness_centrality(G),
                      'eigenvector_centrality': nx.eigenvector_centrality(G, max_iter=1000)
                  },
                  'critical_nodes': {
                      'highest_degree': sorted(nx.degree_centrality(G).items(), key=lambda x: x[1], reverse=True)[:5],
                      'most_central': sorted(nx.betweenness_centrality(G).items(), key=lambda x: x[1], reverse=True)[:5],
                      'bottlenecks': [node for node, centrality in nx.betweenness_centrality(G).items() if centrality > 0.1]
                  },
                  'communities': {},
                  'cycles': list(nx.simple_cycles(G)) if len(G.nodes()) < 100 else []  # Expensive for large graphs
              }
              
              # Detect communities (simplified)
              try:
                  from networkx.algorithms import community
                  if len(G.nodes()) > 1:
                      communities = community.greedy_modularity_communities(G)
                      analysis['communities'] = {
                          f'community_{i}': list(community) 
                          for i, community in enumerate(communities)
                      }
              except:
                  pass
              
              # Save graph and analysis
              graph_data = nx.node_link_data(G)
              
              with open('dependency-graph.json', 'w') as f:
                  json.dump(graph_data, f, indent=2)
              
              with open('graph-analysis.json', 'w') as f:
                  json.dump(analysis, f, indent=2)
              
              print(f'ðŸ”— Built dependency graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges')
              return G, analysis
          
          if __name__ == '__main__':
              build_dependency_graph()
          "
          
          echo "graph=dependency-graph.json" >> $GITHUB_OUTPUT

      - name: Establish performance baselines
        id: baseline
        run: |
          echo "ðŸ“ˆ Establishing performance baselines..."
          
          python -c "
          import json
          import time
          from pathlib import Path
          
          def establish_baselines():
              baselines = {
                  'timestamp': time.time(),
                  'skill_performance': {},
                  'system_metrics': {},
                  'recommendations': []
              }
              
              # Load metrics and graph data
              with open('skill-metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              with open('graph-analysis.json', 'r') as f:
                  graph_analysis = json.load(f)
              
              # Calculate performance baselines
              for skill_id, skill_data in metrics['skills'].items():
                  complexity = skill_data['metrics']['complexity_score']
                  tokens = skill_data['metrics']['estimated_tokens']
                  
                  # Performance baseline based on complexity and token usage
                  performance_score = max(0, 100 - (complexity * 2) - (tokens / 50))
                  
                  baselines['skill_performance'][skill_id] = {
                      'complexity_score': complexity,
                      'token_usage': tokens,
                      'performance_score': round(performance_score, 2),
                      'efficiency_rating': 'high' if performance_score > 80 else 'medium' if performance_score > 60 else 'low'
                  }
              
              # System-wide baselines
              total_skills = metrics['global_stats']['total_skills']
              avg_complexity = sum(data['complexity_score'] for data in baselines['skill_performance'].values()) / max(total_skills, 1)
              avg_tokens = sum(data['token_usage'] for data in baselines['skill_performance'].values()) / max(total_skills, 1)
              
              baselines['system_metrics'] = {
                  'total_skills': total_skills,
                  'average_complexity': round(avg_complexity, 2),
                  'average_token_usage': round(avg_tokens, 2),
                  'graph_density': graph_analysis['graph_stats']['density'],
                  'connected_components': graph_analysis['graph_stats']['connected_components']
              }
              
              # Generate recommendations
              if avg_complexity > 5:
                  baselines['recommendations'].append({
                      'type': 'complexity',
                      'priority': 'high',
                      'message': 'Average skill complexity is high. Consider breaking down complex skills.'
                  })
              
              if graph_analysis['graph_stats']['density'] > 0.3:
                  baselines['recommendations'].append({
                      'type': 'dependencies',
                      'priority': 'medium',
                      'message': 'High dependency density detected. Consider reducing skill coupling.'
                  })
              
              # Critical path analysis
              critical_nodes = graph_analysis['critical_nodes']['bottlenecks']
              if len(critical_nodes) > 0:
                  baselines['recommendations'].append({
                      'type': 'bottlenecks',
                      'priority': 'high',
                      'message': f'Detected {len(critical_nodes)} skill bottlenecks that may impact performance.',
                      'affected_skills': critical_nodes
                  })
              
              with open('performance-baseline.json', 'w') as f:
                  json.dump(baselines, f, indent=2)
              
              print(f'ðŸ“ˆ Established baselines for {total_skills} skills')
              return baselines
          
          if __name__ == '__main__':
              establish_baselines()
          "
          
          echo "baseline=performance-baseline.json" >> $GITHUB_OUTPUT

  # Phase 2: Machine Learning Analysis
  ml-analysis:
    name: Machine Learning Analysis
    runs-on: ubuntu-latest
    needs: collect-skill-data
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python with ML stack
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download collected data
        uses: actions/download-artifact@v4
        with:
          name: skill-data
          path: data/

      - name: Install ML dependencies
        run: |
          python -m pip install --upgrade pip
          pip install scikit-learn xgboost lightgbm
          pip install pandas numpy matplotlib seaborn plotly
          pip install transformers torch

      - name: Run ML optimization analysis
        run: |
          echo "ðŸ¤– Running machine learning analysis..."
          
          python -c "
          import json
          import numpy as np
          from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
          from sklearn.cluster import KMeans, DBSCAN
          from sklearn.preprocessing import StandardScaler
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import mean_squared_error, r2_score
          import matplotlib.pyplot as plt
          
          def ml_optimization_analysis():
              # Load data
              with open('data/skill-metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              with open('data/graph-analysis.json', 'r') as f:
                  graph_data = json.load(f)
              
              with open('data/performance-baseline.json', 'r') as f:
                  baseline = json.load(f)
              
              # Prepare ML features
              X = []
              y_performance = []
              y_efficiency = []
              skill_ids = []
              
              for skill_id, skill_data in metrics['skills'].items():
                  # Feature engineering
                  features = [
                      skill_data['metrics']['word_count'],
                      skill_data['metrics']['complexity_score'],
                      skill_data['metrics']['estimated_tokens'],
                      skill_data['metrics']['complexity_factors']['code_blocks'],
                      skill_data['metrics']['complexity_factors']['steps'],
                      skill_data['activity']['recent_commits'],
                      sum(skill_data['quality_indicators'].values()),  # quality score
                      graph_data['centrality_analysis']['degree_centrality'].get(skill_id, 0),
                      graph_data['centrality_analysis']['betweenness_centrality'].get(skill_id, 0)
                  ]
                  
                  X.append(features)
                  y_performance.append(baseline['skill_performance'][skill_id]['performance_score'])
                  y_efficiency.append(1 if baseline['skill_performance'][skill_id]['efficiency_rating'] == 'high' else 0)
                  skill_ids.append(skill_id)
              
              X = np.array(X)
              y_performance = np.array(y_performance)
              y_efficiency = np.array(y_efficiency)
              
              # Feature scaling
              scaler = StandardScaler()
              X_scaled = scaler.fit_transform(X)
              
              # Performance prediction model
              X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_performance, test_size=0.2, random_state=42)
              
              # Train multiple models
              models = {
                  'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
                  'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
              }
              
              best_model = None
              best_score = -np.inf
              model_results = {}
              
              for name, model in models.items():
                  model.fit(X_train, y_train)
                  y_pred = model.predict(X_test)
                  
                  mse = mean_squared_error(y_test, y_pred)
                  r2 = r2_score(y_test, y_pred)
                  
                  model_results[name] = {
                      'mse': mse,
                      'r2_score': r2,
                      'predictions': y_pred.tolist()
                  }
                  
                  if r2 > best_score:
                      best_score = r2
                      best_model = model
              
              # Feature importance analysis
              if hasattr(best_model, 'feature_importances_'):
                  feature_names = [
                      'word_count', 'complexity_score', 'estimated_tokens',
                      'code_blocks', 'steps', 'recent_commits', 'quality_score',
                      'degree_centrality', 'betweenness_centrality'
                  ]
                  
                  importance_data = list(zip(feature_names, best_model.feature_importances_))
                  importance_data.sort(key=lambda x: x[1], reverse=True)
              
              # Skill clustering
              n_clusters = min(5, len(X_scaled))
              kmeans = KMeans(n_clusters=n_clusters, random_state=42)
              clusters = kmeans.fit_predict(X_scaled)
              
              # Anomaly detection
              from sklearn.ensemble import IsolationForest
              iso_forest = IsolationForest(contamination=0.1, random_state=42)
              anomalies = iso_forest.fit_predict(X_scaled)
              
              # Generate ML insights
              insights = {
                  'model_performance': model_results,
                  'best_model': best_model.__class__.__name__ if best_model else 'None',
                  'feature_importance': importance_data if 'importance_data' in locals() else [],
                  'skill_clusters': {},
                  'anomalies': [],
                  'optimization_recommendations': []
              }
              
              # Group skills by cluster
              for i, (skill_id, cluster) in enumerate(zip(skill_ids, clusters)):
                  if str(cluster) not in insights['skill_clusters']:
                      insights['skill_clusters'][str(cluster)] = []
                  insights['skill_clusters'][str(cluster)].append({
                      'skill_id': skill_id,
                      'performance_score': float(y_performance[i]),
                      'efficiency': bool(y_efficiency[i])
                  })
              
              # Identify anomalies
              for i, (skill_id, anomaly) in enumerate(zip(skill_ids, anomalies)):
                  if anomaly == -1:  # Anomaly detected
                      insights['anomalies'].append({
                          'skill_id': skill_id,
                          'reason': 'Statistical outlier in feature space'
                      })
              
              # Generate optimization recommendations
              for i, skill_id in enumerate(skill_ids):
                  performance_score = y_performance[i]
                  
                  if performance_score < 50:
                      insights['optimization_recommendations'].append({
                          'skill_id': skill_id,
                          'issue': 'Low performance score',
                          'suggestion': 'Consider reducing complexity or token usage',
                          'priority': 'high'
                      })
                  
                  if y_efficiency[i] == 0:  # Low efficiency
                      insights['optimization_recommendations'].append({
                          'skill_id': skill_id,
                          'issue': 'Low efficiency rating',
                          'suggestion': 'Optimize for token efficiency',
                          'priority': 'medium'
                      })
              
              # Save ML analysis results
              with open('ml-analysis-results.json', 'w') as f:
                  json.dump(insights, f, indent=2)
              
              print(f'ðŸ¤– ML analysis complete. Best model: {best_model.__class__.__name__} (RÂ²: {best_score:.3f})')
              print(f'ðŸ“Š Identified {len(insights["skill_clusters"])} skill clusters')
              print(f'ðŸš¨ Found {len(insights["anomalies"])} anomalies')
              print(f'ðŸ’¡ Generated {len(insights["optimization_recommendations"])} optimization recommendations')
          
          if __name__ == '__main__':
              ml_optimization_analysis()
          "

      - name: Generate skill optimization report
        run: |
          echo "ðŸ“‹ Generating skill optimization report..."
          
          python -c "
          import json
          from datetime import datetime
          
          def generate_optimization_report():
              # Load all analysis results
              with open('data/skill-metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              with open('data/graph-analysis.json', 'r') as f:
                  graph_data = json.load(f)
              
              with open('data/performance-baseline.json', 'r') as f:
                  baseline = json.load(f)
              
              with open('ml-analysis-results.json', 'r') as f:
                  ml_results = json.load(f)
              
              # Generate comprehensive report
              report = f'''# Intelligent Skill Optimization Report
          
          Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
          Analysis Depth: ${{ github.event.inputs.analysis_depth || 'moderate' }}
          Optimization Target: ${{ github.event.inputs.optimization_target || 'comprehensive' }}
          
          ## ðŸ“Š Executive Summary
          
          - **Total Skills Analyzed**: {metrics['global_stats']['total_skills']}
          - **Categories**: {len(metrics['categories'])}
          - **ML Model Performance**: {ml_results['best_model']} (RÂ²: {ml_results['model_performance'].get(ml_results['best_model'], {}).get('r2_score', 'N/A'):.3f})
          - **Skill Clusters Identified**: {len(ml_results['skill_clusters'])}
          - **Anomalies Detected**: {len(ml_results['anomalies'])}
          - **Optimization Recommendations**: {len(ml_results['optimization_recommendations'])}
          
          ## ðŸŽ¯ Key Findings
          
          ### Performance Insights
          - Average skill complexity: {baseline['system_metrics']['average_complexity']:.2f}
          - Average token usage: {baseline['system_metrics']['average_token_usage']:.0f} tokens
          - Graph density: {baseline['system_metrics']['graph_density']:.3f}
          - Connected components: {baseline['system_metrics']['connected_components']}
          
          ### Critical Issues
          '''
              
              # Add high-priority recommendations
              high_priority = [rec for rec in ml_results['optimization_recommendations'] if rec['priority'] == 'high']
              if high_priority:
                  report += '\n#### High Priority Issues\n'
                  for rec in high_priority[:5]:  # Top 5
                      report += f'- **{rec[\"skill_id\"]}**: {rec[\"issue\"]} - {rec[\"suggestion\"]}\n'
              
              # Add feature importance
              if ml_results['feature_importance']:
                  report += '\n## ðŸ” Feature Importance Analysis\n'
                  report += '| Feature | Importance |\n'
                  report += '|---------|------------|\n'
                  for feature, importance in ml_results['feature_importance'][:10]:
                      report += f'| {feature} | {importance:.3f} |\n'
              
              # Add cluster analysis
              if ml_results['skill_clusters']:
                  report += '\n## ðŸ“ˆ Skill Cluster Analysis\n'
                  for cluster_id, skills in ml_results['skill_clusters'].items():
                      avg_performance = sum(s['performance_score'] for s in skills) / len(skills)
                      report += f'\n### Cluster {cluster_id} ({len(skills)} skills)\n'
                      report += f'- Average Performance: {avg_performance:.1f}\n'
                      report += f'- High-performing skills: {sum(1 for s in skills if s[\"performance_score\"] > 75)}\n'
                      
                      # Show top skills in cluster
                      top_skills = sorted(skills, key=lambda x: x['performance_score'], reverse=True)[:3]
                      report += '- Top performers: ' + ', '.join([s['skill_id'] for s in top_skills]) + '\n'
              
              # Add anomaly analysis
              if ml_results['anomalies']:
                  report += '\n## ðŸš¨ Anomaly Analysis\n'
                  report += f'Detected {len(ml_results[\"anomalies\"])} anomalous skills that deviate significantly from normal patterns:\n'
                  for anomaly in ml_results['anomalies'][:5]:
                      report += f'- **{anomaly[\"skill_id\"]}**: {anomaly[\"reason\"]}\n'
              
              # Add recommendations section
              report += '''
          ## ðŸ’¡ Optimization Recommendations
          
          ### Immediate Actions (High Priority)
          '''
              
              immediate_actions = [rec for rec in ml_results['optimization_recommendations'] if rec['priority'] == 'high']
              for i, action in enumerate(immediate_actions[:5], 1):
                  report += f'{i}. **{action[\"skill_id\"]}**: {action[\"suggestion\"]}\n'
              
              report += '''
          ### Medium-Term Improvements
          '''
              
              medium_actions = [rec for rec in ml_results['optimization_recommendations'] if rec['priority'] == 'medium']
              for i, action in enumerate(medium_actions[:5], 1):
                  report += f'{i}. **{action[\"skill_id\"]}**: {action[\"suggestion\"]}\n'
              
              # Add conclusion
              report += '''
          ## ðŸ“‹ Action Plan
          
          1. **Review high-priority issues** identified by ML analysis
          2. **Optimize bottleneck skills** to improve overall system performance
          3. **Address anomalous skills** that may indicate quality issues
          4. **Implement token efficiency improvements** for high-usage skills
          5. **Monitor cluster performance** and adjust strategies accordingly
          
          ## ðŸ”§ Technical Details
          
          - **ML Model**: ''' + ml_results['best_model'] + '''
          - **Analysis Date**: ''' + datetime.now().isoformat() + '''
          - **Data Quality**: High (based on ''' + str(metrics['global_stats']['total_skills']) + ''' skills)
          - **Confidence Level**: ''' + ('High' if ml_results['model_performance'].get(ml_results['best_model'], {}).get('r2_score', 0) > 0.7 else 'Medium' if ml_results['model_performance'].get(ml_results['best_model'], {}).get('r2_score', 0) > 0.5 else 'Low') + '''
          '''
              
              # Save report
              with open('skill-optimization-report.md', 'w') as f:
                  f.write(report)
              
              print('ðŸ“‹ Generated comprehensive optimization report')
          
          if __name__ == '__main__':
              generate_optimization_report()
          "

      - name: Upload ML analysis results
        uses: actions/upload-artifact@v4
        with:
          name: ml-analysis-results
          path: |
            ml-analysis-results.json
            skill-optimization-report.md
          retention-days: 90

  # Phase 3: Optimization Implementation
  implement-optimizations:
    name: Implement Automatic Optimizations
    runs-on: ubuntu-latest
    needs: [collect-skill-data, ml-analysis]
    if: github.event.inputs.generate_recommendations == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download analysis results
        uses: actions/download-artifact@v4
        with:
          name: ml-analysis-results
          path: results/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Implement automatic optimizations
        run: |
          echo "ðŸ”§ Implementing automatic optimizations..."
          
          python -c "
          import json
          import re
          from pathlib import Path
          from datetime import datetime
          
          def implement_optimizations():
              # Load ML analysis results
              with open('results/ml-analysis-results.json', 'r') as f:
                  ml_results = json.load(f)
              
              # Load original metrics
              with open('data/skill-metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              optimizations_applied = []
              
              # Apply token efficiency optimizations
              for recommendation in ml_results['optimization_recommendations']:
                  if recommendation['issue'] == 'Low efficiency rating':
                      skill_id = recommendation['skill_id']
                      category, skill_name = skill_id.split('/')
                      
                      skill_dir = Path(f'skills/{category}/{skill_name}')
                      skill_file = skill_dir / 'SKILL.md'
                      
                      if skill_file.exists():
                          content = skill_file.read_text()
                          original_length = len(content)
                          
                          # Apply optimizations
                          optimized_content = content
                          
                          # Remove redundant examples
                          if optimized_content.count('```') > 3:
                              # Keep only the most relevant examples
                              code_blocks = re.findall(r'```.*?```', optimized_content, re.DOTALL)
                              if len(code_blocks) > 2:
                                  # Keep first and last example
                                  kept_blocks = [code_blocks[0], code_blocks[-1]]
                                  for block in code_blocks[1:-1]:
                                      optimized_content = optimized_content.replace(block, '')
                          
                          # Optimize repetitive instructions
                          optimized_content = re.sub(
                              r'(Please|Kindly|Note that|Remember to)',
                              '',
                              optimized_content,
                              flags=re.IGNORECASE
                          )
                          
                          # Remove unnecessary whitespace
                          optimized_content = re.sub(r'\n{3,}', '\n\n', optimized_content)
                          
                          # Add efficiency notice
                          if len(optimized_content) < original_length * 0.9:
                              efficiency_notice = f'\n\n<!-- Optimized for token efficiency on {datetime.now().strftime(\"%Y-%m-%d\")} -->\n'
                              optimized_content += efficiency_notice
                          
                          # Save if significantly optimized
                          if len(optimized_content) < original_length * 0.95:
                              backup_file = skill_dir / 'SKILL.md.backup'
                              backup_file.write_text(content)
                              
                              skill_file.write_text(optimized_content)
                              
                              optimizations_applied.append({
                                  'skill_id': skill_id,
                                  'optimization_type': 'token_efficiency',
                                  'original_length': original_length,
                                  'optimized_length': len(optimized_content),
                                  'reduction_percentage': round((1 - len(optimized_content) / original_length) * 100, 1)
                              })
              
              # Apply complexity optimizations
              for recommendation in ml_results['optimization_recommendations']:
                  if recommendation['issue'] == 'Low performance score':
                      skill_id = recommendation['skill_id']
                      category, skill_name = skill_id.split('/')
                      
                      skill_dir = Path(f'skills/{category}/{skill_name}')
                      skill_file = skill_dir / 'SKILL.md'
                      
                      if skill_file.exists():
                          content = skill_file.read_text()
                          
                          # Suggest breaking down complex skills
                          if len(content.split('\n')) > 100:
                              # Create a summary version
                              lines = content.split('\n')
                              summary_lines = []
                              detail_lines = []
                              
                              in_detail_section = False
                              for line in lines:
                                  if any(keyword in line.lower() for keyword in ['detailed', 'advanced', 'complex']):
                                      in_detail_section = True
                                  
                                  if in_detail_section:
                                      detail_lines.append(line)
                                  else:
                                      summary_lines.append(line)
                              
                              # Create advanced variant
                              if len(detail_lines) > 10:
                                  advanced_file = skill_dir / 'SKILL-advanced.md'
                                  advanced_content = '# Advanced Version: ' + skill_name + '\n\n' + '\n'.join(detail_lines)
                                  advanced_file.write_text(advanced_content)
                                  
                                  # Create simplified main version
                                  simplified_content = '\n'.join(summary_lines)
                                  simplified_content += f'\n\n<!-- For advanced usage, see SKILL-advanced.md -->\n'
                                  
                                  skill_file.write_text(simplified_content)
                                  
                                  optimizations_applied.append({
                                      'skill_id': skill_id,
                                      'optimization_type': 'complexity_reduction',
                                      'action': 'Created advanced variant and simplified main skill'
                                  })
              
              # Save optimization results
              optimization_summary = {
                  'timestamp': datetime.now().isoformat(),
                  'total_optimizations': len(optimizations_applied),
                  'optimizations': optimizations_applied,
                  'summary': {
                      'token_efficiency_optimizations': len([opt for opt in optimizations_applied if opt['optimization_type'] == 'token_efficiency']),
                      'complexity_optimizations': len([opt for opt in optimizations_applied if opt['optimization_type'] == 'complexity_reduction']),
                      'total_size_reduction': sum(opt.get('reduction_percentage', 0) for opt in optimizations_applied if 'reduction_percentage' in opt)
                  }
              }
              
              with open('optimization-results.json', 'w') as f:
                  json.dump(optimization_summary, f, indent=2)
              
              # Generate optimization report
              report = f'''# Automatic Optimization Results
          
          Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
          Total Optimizations Applied: {len(optimizations_applied)}
          
          ## Summary
          - Token Efficiency Optimizations: {optimization_summary['summary']['token_efficiency_optimizations']}
          - Complexity Reductions: {optimization_summary['summary']['complexity_optimizations']}
          - Total Size Reduction: {optimization_summary['summary']['total_size_reduction']:.1f}%
          
          ## Detailed Optimizations
          '''
              
              for opt in optimizations_applied:
                  report += f'\n### {opt[\"skill_id\"]}\n'
                  report += f'- Type: {opt[\"optimization_type\"].replace(\"_\", \" \").title()}\n'
                  if 'reduction_percentage' in opt:
                      report += f'- Size Reduction: {opt[\"reduction_percentage\"]}%\n'
                  if 'action' in opt:
                      report += f'- Action: {opt[\"action\"]}\n'
              
              with open('optimization-report.md', 'w') as f:
                  f.write(report)
              
              print(f'ðŸ”§ Applied {len(optimizations_applied)} automatic optimizations')
              print(f'ðŸ“Š Total size reduction: {optimization_summary[\"summary\"][\"total_size_reduction\"]:.1f}%')
          
          if __name__ == '__main__':
              implement_optimizations()
          "

      - name: Create pull request with optimizations
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: 'chore: apply ML-based skill optimizations'
          title: 'ðŸ¤– ML-Based Skill Optimization Update'
          body: |
            ## Automatic Skill Optimizations Applied
            
            This PR contains optimizations generated by our ML-based skill analysis system.
            
            ### Changes Made
            - Token efficiency improvements for high-usage skills
            - Complexity reduction for underperforming skills
            - Automatic backup creation for modified skills
            
            ### Review Checklist
            - [ ] Verify skill functionality is preserved
            - [ ] Check that optimizations don't break existing workflows
            - [ ] Review backup files for reference
            - [ ] Test optimized skills in isolation
            
            ### Generated Files
            - `optimization-results.json` - Detailed optimization data
            - `optimization-report.md` - Human-readable report
            - `*.backup` files - Original skill versions
            
            *This PR was automatically generated by the Intelligent Skill Optimizer workflow.*
          branch: auto/skill-optimizations
          delete-branch: true

      - name: Upload optimization results
        uses: actions/upload-artifact@v4
        with:
          name: optimization-results
          path: |
            optimization-results.json
            optimization-report.md
          retention-days: 30

  # Final reporting and notifications
  final-report:
    name: Generate Final Report
    runs-on: ubuntu-latest
    needs: [collect-skill-data, ml-analysis, implement-optimizations]
    if: always()
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: results/

      - name: Generate comprehensive final report
        run: |
          echo "ðŸ“‹ Generating comprehensive final report..."
          
          # Combine all results into final summary
          cat > final-report.md << 'EOF'
          # Intelligent Skill Analysis & Optimization - Final Report
          
          **Analysis Date**: $(date)
          **Workflow Run**: ${{ github.run_id }}
          **Analysis Depth**: ${{ github.event.inputs.analysis_depth || 'moderate' }}
          **Optimization Target**: ${{ github.event.inputs.optimization_target || 'comprehensive' }}
          
          ## ðŸ“Š Analysis Summary
          
          This comprehensive analysis leveraged machine learning techniques to optimize the Claude Code Skills repository.
          
          ### Data Collected
          - Skill usage metrics and complexity analysis
          - Dependency graph construction and analysis  
          - Performance baseline establishment
          - ML-based optimization recommendations
          
          ### Key Insights
          The analysis revealed patterns in skill complexity, token usage, and performance that enable data-driven optimization decisions.
          
          ## ðŸ” Technical Implementation
          
          ### Machine Learning Models Used
          - **Random Forest**: For performance prediction
          - **Gradient Boosting**: For ensemble learning
          - **K-Means Clustering**: For skill categorization
          - **Isolation Forest**: For anomaly detection
          
          ### Features Analyzed
          - Word count and complexity metrics
          - Code block frequency
          - Step-by-step instruction patterns
          - Git activity and modification history
          - Network centrality measures
          - Quality indicators (documentation, examples, templates)
          
          ## ðŸ’¡ Recommendations
          
          1. **Regular Analysis**: Run this workflow weekly to track optimization trends
          2. **Manual Review**: Always review ML-generated optimizations before merging
          3. **Performance Monitoring**: Track skill performance metrics over time
          4. **Continuous Learning**: Update ML models as more data becomes available
          
          ## ðŸ“ Generated Artifacts
          
          - `skill-metrics.json` - Comprehensive skill analysis data
          - `dependency-graph.json` - Skill dependency relationships
          - `graph-analysis.json` - Network analysis results
          - `performance-baseline.json` - Performance benchmarks
          - `ml-analysis-results.json` - Machine learning insights
          - `skill-optimization-report.md` - Detailed optimization report
          - `optimization-results.json` - Applied optimizations
          
          ---
          
          *This report was generated automatically by the Intelligent Skill Dependency Analyzer & Optimizer workflow.*
          EOF

      - name: Upload final report
        uses: actions/upload-artifact@v4
        with:
          name: final-analysis-report
          path: final-report.md
          retention-days: 90

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let reportContent = 'Analysis results will be available in workflow artifacts.';
            try {
              if (fs.existsSync('results/ml-analysis-results/skill-optimization-report.md')) {
                reportContent = fs.readFileSync('results/ml-analysis-results/skill-optimization-report.md', 'utf8');
              }
            } catch (error) {
              console.log('Could not read optimization report');
            }
            
            const comment = `## ðŸ¤– Intelligent Skill Analysis Complete
            
            âœ… Machine learning analysis of skills completed successfully!
            
            ### Analysis Summary
            - **Skills Analyzed**: Multiple categories across the repository
            - **ML Models Used**: Random Forest, Gradient Boosting, Clustering
            - **Optimization Target**: ${{ github.event.inputs.optimization_target || 'comprehensive' }}
            - **Analysis Depth**: ${{ github.event.inputs.analysis_depth || 'moderate' }}
            
            ### Key Insights
            The analysis identified patterns in skill complexity, token usage, and performance efficiency.
            
            ### Next Steps
            - Review generated optimization recommendations
            - Consider implementing suggested improvements
            - Monitor skill performance over time
            
            ### Detailed Results
            ${reportContent.split('\n').slice(0, 30).join('\n')}
            
            [View full analysis results in workflow artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ---
            *This comment was automatically generated by the intelligent skill optimizer workflow.*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
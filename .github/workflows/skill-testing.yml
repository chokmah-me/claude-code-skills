name: Skill Testing & Validation

on:
  pull_request:
    paths:
      - 'skills/**/SKILL.md'
      - 'skills/**/test*'
      - '.github/workflows/skill-testing.yml'
  push:
    branches:
      - main
      - develop
  schedule:
    # Run comprehensive tests weekly
    - cron: '0 2 * * 1'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        type: choice
        options:
          - 'quick'
          - 'comprehensive'
          - 'meta-skills-only'
          - 'new-skills-only'
        default: 'quick'

env:
  PYTHON_VERSION: '3.11'
  TEST_TIMEOUT: '300'  # 5 minutes per skill

jobs:
  analyze-skills:
    name: Analyze Skills
    runs-on: ubuntu-latest
    outputs:
      changed-skills: ${{ steps.changes.outputs.skills }}
      test-strategy: ${{ steps.strategy.outputs.strategy }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine test strategy
        id: strategy
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            STRATEGY="${{ github.event.inputs.test_type }}"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            STRATEGY="comprehensive"
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            STRATEGY="quick"
          else
            STRATEGY="quick"
          fi
          
          echo "strategy=$STRATEGY" >> $GITHUB_OUTPUT
          echo "[TARGET] Test strategy: $STRATEGY"

      - name: Get changed skills
        id: changes
        if: github.event_name == 'pull_request'
        run: |
          echo "[SCAN] Finding changed skills..."
          CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }})
          
          CHANGED_SKILLS="[]"
          if [ -n "$CHANGED_FILES" ]; then
            # Extract skill names from changed files
            SKILLS=$(echo "$CHANGED_FILES" | grep -E '^skills/[^/]+/[^/]+/' | sed 's|^skills/||' | cut -d'/' -f1-2 | sort -u | jq -R -s -c 'split(\"\\n\")[:-1]')
            if [ -n "$SKILLS" ] && [ "$SKILLS" != "[]" ]; then
              CHANGED_SKILLS="$SKILLS"
            fi
          fi
          
          echo "skills=$CHANGED_SKILLS" >> $GITHUB_OUTPUT
          echo "Changed skills: $CHANGED_SKILLS"

  test-meta-skills:
    name: Test Meta Skills
    runs-on: ubuntu-latest
    needs: analyze-skills
    strategy:
      matrix:
        skill: 
          - session-snapshot
          - skill-extractor
          - skill-recommendation-engine
          - claude-startup-integration
          - startup-skill-showcase
          - manifest-generator
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml markdown beautifulsoup4

      - name: Test ${{ matrix.skill }}
        timeout-minutes: 10
        run: |
          echo "[TEST] Testing meta skill: ${{ matrix.skill }}"
          SKILL_DIR="skills/meta/${{ matrix.skill }}"
          
          if [ ! -d "$SKILL_DIR" ]; then
            echo "[ERROR] Skill directory not found: $SKILL_DIR"
            exit 1
          fi
          
          # Test SKILL.md parsing
          if [ -f "$SKILL_DIR/SKILL.md" ]; then
            echo "[LIST] Testing SKILL.md parsing..."
            python -c "
import re
from pathlib import Path

skill_file = Path('$SKILL_DIR/SKILL.md')
if skill_file.exists():
    content = skill_file.read_text()
    
    # Check for required elements
    required_patterns = [
        r'Purpose:.*',
        r'Usage:.*',
        r'Examples?:.*',
        r'/${{ matrix.skill }}'
    ]
    
    missing_patterns = []
    for pattern in required_patterns:
        if not re.search(pattern, content, re.IGNORECASE):
            missing_patterns.append(pattern)
    
    if missing_patterns:
        print(f'[WARN]  Missing patterns: {missing_patterns}')
    else:
        print('[OK] SKILL.md structure is valid')
    
    # Check for token efficiency mentions (for meta skills)
    if 'token' in content.lower():
        print('[OK] Token efficiency mentioned')
    else:
        print('[WARN]  Consider mentioning token efficiency')
"
          fi
          
          # Test README.md if exists
          if [ -f "$SKILL_DIR/README.md" ]; then
            echo " Testing README.md..."
            python -c "
import re
from pathlib import Path

readme_file = Path('$SKILL_DIR/README.md')
if readme_file.exists():
    content = readme_file.read_text()
    
    # Check for comprehensive documentation
    sections = ['overview', 'usage', 'examples', 'installation']
    found_sections = [s for s in sections if s in content.lower()]
    
    print(f' Found sections: {found_sections}')
    
    # Check for code examples
    code_blocks = len(re.findall(r'```', content))
    print(f'[LOG] Code blocks: {code_blocks}')
    
    if len(found_sections) >= 3 and code_blocks >= 2:
        print('[OK] README.md is comprehensive')
    else:
        print('[WARN]  README.md could be more detailed')
"
          fi
          
          # Test template.md if exists
          if [ -f "$SKILL_DIR/template.md" ]; then
            echo "[FILE] Testing template.md..."
            python -c "
from pathlib import Path

template_file = Path('$SKILL_DIR/template.md')
if template_file.exists():
    content = template_file.read_text()
    
    # Check for template characteristics
    placeholders = content.count('{{') + content.count('}}')
    todos = content.upper().count('TODO')
    
    print(f'[TARGET] Template placeholders: {placeholders}')
    print(f'  TODO items: {todos}')
    
    if placeholders > 0 or todos > 0:
        print('[OK] Template has customization points')
    else:
        print('[WARN]  Template might benefit from placeholders')
"
          fi
          
          echo "[OK] ${{ matrix.skill }} testing completed"

  test-skill-functionality:
    name: Test Skill Functionality
    runs-on: ubuntu-latest
    needs: analyze-skills
    strategy:
      matrix:
        skill-category: [analysis, development, git]
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml markdown

      - name: Test ${{ matrix.skill-category }} skills
        timeout-minutes: 15
        run: |
          echo "[TEST] Testing ${{ matrix.skill-category }} skills..."
          CATEGORY_DIR="skills/${{ matrix.skill-category }}"
          
          if [ ! -d "$CATEGORY_DIR" ]; then
            echo "[ERROR] Category directory not found: $CATEGORY_DIR"
            exit 1
          fi
          
          # Find all skills in category
          SKILLS=$(find "$CATEGORY_DIR" -mindepth 1 -maxdepth 1 -type d | xargs -I {} basename {} | sort)
          
          if [ -z "$SKILLS" ]; then
            echo "[WARN]  No skills found in ${{ matrix.skill-category }} category"
            exit 0
          fi
          
          echo "Found skills: $SKILLS"
          
          # Test each skill
          for skill in $SKILLS; do
            echo "[SCAN] Testing $skill..."
            SKILL_DIR="$CATEGORY_DIR/$skill"
            
            # Test SKILL.md exists and is valid
            if [ -f "$SKILL_DIR/SKILL.md" ]; then
              python -c "
import re
from pathlib import Path

skill_file = Path('$SKILL_DIR/SKILL.md')
try:
    content = skill_file.read_text()
    
    # Basic validation
    if len(content) < 100:
        print(f'[WARN]  SKILL.md seems very short ({len(content)} chars)')
    
    # Check for skill invocation
    skill_name = '$skill'
    if f'/{skill_name}' in content:
        print(f'[OK] Found skill invocation: /{skill_name}')
    else:
        print(f'[WARN]  Missing skill invocation pattern: /{skill_name}')
    
    # Check for purpose statement
    if 'purpose:' in content.lower():
        print('[OK] Found purpose statement')
    else:
        print('[WARN]  Missing purpose statement')
    
    print(f'[OK] {skill_name} basic validation passed')
    
except Exception as e:
    print(f'[ERROR] Error testing {skill_name}: {e}')
"
            else
              echo "[ERROR] Missing SKILL.md for $skill"
            fi
          done
          
          echo "[OK] ${{ matrix.skill-category }} skills testing completed"

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: analyze-skills
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Analyze skill complexity
        run: |
          echo "[STATS] Analyzing skill complexity and token usage..."
          
          python -c "
import re
from pathlib import Path
import json

def analyze_skill_complexity():
    '''Analyze skill complexity for token usage estimation'''
    complexity_report = {
        'skills_analyzed': 0,
        'token_estimates': {},
        'complexity_distribution': {'low': 0, 'medium': 0, 'high': 0},
        'recommendations': []
    }
    
    skills_dir = Path('skills')
    if not skills_dir.exists():
        print('[ERROR] Skills directory not found')
        return
    
    for category_dir in skills_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            category = category_dir.name
            
            for skill_dir in category_dir.iterdir():
                if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                    skill_name = skill_dir.name
                    skill_file = skill_dir / 'SKILL.md'
                    
                    if skill_file.exists():
                        complexity_report['skills_analyzed'] += 1
                        
                        try:
                            content = skill_file.read_text()
                            
                            # Estimate complexity based on content
                            word_count = len(content.split())
                            code_blocks = len(re.findall(r'```', content))
                            steps = len(re.findall(r'^[0-9]+\.', content, re.MULTILINE))
                            bullet_points = len(re.findall(r'^[-*+]', content, re.MULTILINE))
                            
                            # Token estimation (rough approximation)
                            # ~1.3 tokens per word, plus extra for code blocks and formatting
                            estimated_tokens = int(word_count * 1.3 + code_blocks * 50 + steps * 10)
                            
                            complexity_report['token_estimates'][f'{category}/{skill_name}'] = {
                                'estimated_tokens': estimated_tokens,
                                'word_count': word_count,
                                'code_blocks': code_blocks,
                                'steps': steps,
                                'category': category
                            }
                            
                            # Categorize complexity
                            if estimated_tokens < 500:
                                complexity_level = 'low'
                            elif estimated_tokens < 1500:
                                complexity_level = 'medium'
                            else:
                                complexity_level = 'high'
                            
                            complexity_report['complexity_distribution'][complexity_level] += 1
                            
                            # Add recommendations
                            if estimated_tokens > 2000:
                                complexity_report['recommendations'].append(
                                    f'{category}/{skill_name}: Consider splitting into smaller skills (>{estimated_tokens} tokens)'
                                )
                            elif estimated_tokens > 1500:
                                complexity_report['recommendations'].append(
                                    f'{category}/{skill_name}: Consider optimization for token efficiency ({estimated_tokens} tokens)'
                                )
                            
                            print(f'[STATS] {category}/{skill_name}: ~{estimated_tokens} tokens ({complexity_level})')
                            
                        except Exception as e:
                            print(f'[ERROR] Error analyzing {skill_name}: {e}')
    
    # Save report
    with open('complexity-analysis.json', 'w') as f:
        json.dump(complexity_report, f, indent=2)
    
    # Generate summary
    print(f'\\n Complexity Analysis Summary:')
    print(f'  Skills analyzed: {complexity_report[\"skills_analyzed\"]}')
    print(f'  Low complexity: {complexity_report[\"complexity_distribution\"][\"low\"]}')
    print(f'  Medium complexity: {complexity_report[\"complexity_distribution\"][\"medium\"]}')
    print(f'  High complexity: {complexity_report[\"complexity_distribution\"][\"high\"]}')
    
    if complexity_report['recommendations']:
        print(f'\\n[INFO] Recommendations:')
        for rec in complexity_report['recommendations']:
            print(f'  - {rec}')
    
    print('[OK] Complexity analysis completed')

analyze_skill_complexity()
          "

      - name: Test skill dependencies
        run: |
          echo "[LINK] Testing skill dependencies..."
          
          python -c "
import re
from pathlib import Path
import json

def analyze_dependencies():
    '''Analyze dependencies between skills'''
    dependency_map = {}
    
    skills_dir = Path('skills')
    if not skills_dir.exists():
        print('[ERROR] Skills directory not found')
        return
    
    # First pass: collect all skill names
    all_skills = set()
    for category_dir in skills_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            for skill_dir in category_dir.iterdir():
                if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                    all_skills.add(skill_dir.name)
    
    print(f'[LIST] Found {len(all_skills)} total skills')
    
    # Second pass: analyze dependencies
    for category_dir in skills_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            category = category_dir.name
            
            for skill_dir in category_dir.iterdir():
                if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                    skill_name = skill_dir.name
                    skill_file = skill_dir / 'SKILL.md'
                    
                    if skill_file.exists():
                        try:
                            content = skill_file.read_text()
                            
                            # Find references to other skills
                            dependencies = set()
                            
                            # Look for skill invocations
                            for other_skill in all_skills:
                                if other_skill != skill_name:
                                    # Look for skill invocation patterns
                                    patterns = [
                                        f'/{other_skill}',
                                        f'{other_skill}',
                                        f'``{other_skill}``'
                                    ]
                                    
                                    for pattern in patterns:
                                        if pattern in content:
                                            dependencies.add(other_skill)
                                            break
                            
                            if dependencies:
                                dependency_map[f'{category}/{skill_name}'] = list(dependencies)
                                print(f'[LINK] {category}/{skill_name} depends on: {', '.join(dependencies)}')
                            
                        except Exception as e:
                            print(f'[ERROR] Error analyzing dependencies for {skill_name}: {e}')
    
    # Save dependency map
    with open('skill-dependencies.json', 'w') as f:
        json.dump(dependency_map, f, indent=2)
    
    # Generate dependency report
    print(f'\\n[STATS] Dependency Analysis:')
    print(f'  Skills with dependencies: {len(dependency_map)}')
    
    if dependency_map:
        # Find most depended-upon skills
        dependency_counts = {}
        for deps in dependency_map.values():
            for dep in deps:
                dependency_counts[dep] = dependency_counts.get(dep, 0) + 1
        
        most_depended = sorted(dependency_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f'  Most depended-upon skills:')
        for skill, count in most_depended:
            print(f'    - {skill}: {count} dependencies')
    
    print('[OK] Dependency analysis completed')

analyze_dependencies()
          "

      - name: Upload analysis artifacts
        uses: actions/upload-artifact@v4
        with:
          name: skill-analysis-report
          path: |
            complexity-analysis.json
            skill-dependencies.json
          retention-days: 30

  security-analysis:
    name: Security Analysis
    runs-on: ubuntu-latest
    needs: analyze-skills
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run security checks
        run: |
          echo " Running security analysis..."
          
          # Check for potential security issues in skill definitions
          python -c "
import re
from pathlib import Path

def security_scan():
    '''Basic security scan for skill definitions'''
    security_issues = []
    
    skills_dir = Path('skills')
    if not skills_dir.exists():
        print('[ERROR] Skills directory not found')
        return
    
    dangerous_patterns = [
        (r'eval\s*\(', 'Use of eval() function'),
        (r'exec\s*\(', 'Use of exec() function'),
        (r'__import__', 'Dynamic imports'),
        (r'subprocess\.call', 'Subprocess calls'),
        (r'os\.system', 'System command execution'),
        (r'input\s*\(', 'User input without validation'),
        (r'request\.get\s*\(.*\$\{', 'Potential injection in requests'),
        (r'curl.*\$\{', 'Potential injection in curl commands')
    ]
    
    for category_dir in skills_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            category = category_dir.name
            
            for skill_dir in category_dir.iterdir():
                if skill_dir.is_dir() and not skill_dir.name.startswith('.'):
                    skill_name = skill_dir.name
                    skill_file = skill_dir / 'SKILL.md'
                    
                    if skill_file.exists():
                        try:
                            content = skill_file.read_text()
                            
                            for pattern, description in dangerous_patterns:
                                if re.search(pattern, content, re.IGNORECASE):
                                    security_issues.append({
                                        'skill': f'{category}/{skill_name}',
                                        'issue': description,
                                        'severity': 'high' if 'eval' in pattern or 'exec' in pattern else 'medium'
                                    })
                            
                            # Check for hardcoded secrets (basic patterns)
                            secret_patterns = [
                                (r'[\"\']?[Aa][Pp][Ii][_\-]?[Kk][Ee][Yy][\"\']?\s*[:=]\s*[\"\'][^\"\']{16,}[\"\']', 'Potential API key'),
                                (r'[\"\']?[Pp][Aa][Ss][Ss][Ww][Oo][Rr][Dd][\"\']?\s*[:=]\s*[\"\'][^\"\']{8,}[\"\']', 'Potential password'),
                                (r'[\"\']?[Tt][Oo][Kk][Ee][Nn][\"\']?\s*[:=]\s*[\"\'][^\"\']{16,}[\"\']', 'Potential token')
                            ]
                            
                            for pattern, description in secret_patterns:
                                matches = re.findall(pattern, content)
                                for match in matches:
                                    security_issues.append({
                                        'skill': f'{category}/{skill_name}',
                                        'issue': f'{description} detected',
                                        'severity': 'high'
                                    })
                            
                        except Exception as e:
                            print(f'[ERROR] Error scanning {skill_name}: {e}')
    
    # Report findings
    if security_issues:
        print(' Security Issues Found:')
        high_severity = [issue for issue in security_issues if issue['severity'] == 'high']
        medium_severity = [issue for issue in security_issues if issue['severity'] == 'medium']
        
        if high_severity:
            print(f'\n[ERROR] High Severity ({len(high_severity)}):')
            for issue in high_severity:
                print(f'  - {issue[\"skill\"]}: {issue[\"issue\"]}')
        
        if medium_severity:
            print(f'\n[WARN]  Medium Severity ({len(medium_severity)}):')
            for issue in medium_severity:
                print(f'  - {issue[\"skill\"]}: {issue[\"issue\"]}')
        
        return False
    else:
        print('[OK] No security issues detected')
        return True

security_scan()
          "

      - name: Check dependencies for vulnerabilities
        run: |
          echo "[SCAN] Checking for vulnerable dependencies..."
          
          # Check if we have any dependency files
          if [ -f "requirements.txt" ] || [ -f "Pipfile" ] || [ -f "pyproject.toml" ]; then
            echo "[LIST] Found dependency files - would run vulnerability scan here"
            # This would integrate with tools like safety, snyk, etc.
          else
            echo "[OK] No external dependencies to check"
          fi

  generate-test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [test-meta-skills, test-skill-functionality, performance-analysis, security-analysis]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/

      - name: Generate comprehensive test report
        run: |
          echo "[STATS] Generating comprehensive test report..."
          
          python -c "
import json
from pathlib import Path
from datetime import datetime

def generate_test_report():
    '''Generate comprehensive test report'''
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_run': '${{ github.run_id }}',
        'repository': '${{ github.repository }}',
        'summary': {
            'total_jobs': 4,
            'passed_jobs': 0,
            'failed_jobs': 0,
            'warnings': 0
        },
        'results': {}
    }
    
    # Job results from GitHub Actions context
    job_results = {
        'test-meta-skills': '${{ needs.test-meta-skills.result }}',
        'test-skill-functionality': '${{ needs.test-skill-functionality.result }}',
        'performance-analysis': '${{ needs.performance-analysis.result }}',
        'security-analysis': '${{ needs.security-analysis.result }}'
    }
    
    for job, result in job_results.items():
        report['results'][job] = {
            'status': result,
            'passed': result == 'success',
            'failed': result == 'failure',
            'skipped': result == 'skipped'
        }
        
        if result == 'success':
            report['summary']['passed_jobs'] += 1
        elif result == 'failure':
            report['summary']['failed_jobs'] += 1
    
    report['summary']['success_rate'] = (report['summary']['passed_jobs'] / report['summary']['total_jobs']) * 100
    
    # Save report
    with open('test-report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    # Generate markdown report
    with open('TEST_REPORT.md', 'w') as f:
        f.write('# Claude Code Skills Test Report\n\n')
        f.write(f'**Generated**: {report[\"timestamp\"]}\n')
        f.write(f'**Test Run**: {report[\"test_run\"]}\n\n')
        
        f.write('## [STATS] Summary\n\n')
        f.write(f'- **Total Jobs**: {report[\"summary\"][\"total_jobs\"]}\n')
        f.write(f'- **Passed Jobs**: {report[\"summary\"][\"passed_jobs\"]}\n')
        f.write(f'- **Failed Jobs**: {report[\"summary\"][\"failed_jobs\"]}\n')
        f.write(f'- **Success Rate**: {report[\"summary\"][\"success_rate\"]:.1f}%\n\n')
        
        f.write('## [SCAN] Job Results\n\n')
        for job, result in report['results'].items():
            status_icon = '[OK]' if result['passed'] else '[ERROR]' if result['failed'] else 'тн'
            f.write(f'### {status_icon} {job.replace(\"-\", \" \").title()}\n')
            f.write(f'**Status**: {result[\"status\"]}\n\n')
    
    print('[OK] Test report generated')
    print(f'[STATS] Overall success rate: {report[\"summary\"][\"success_rate\"]:.1f}%')

generate_test_report()
          "

      - name: Upload test report
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: |
            test-report.json
            TEST_REPORT.md
          retention-days: 90

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let reportContent = 'Test report will be available in workflow artifacts.';
            try {
              if (fs.existsSync('TEST_REPORT.md')) {
                reportContent = fs.readFileSync('TEST_REPORT.md', 'utf8');
              }
            } catch (error) {
              console.log('Could not read test report');
            }
            
            const comment = `## [TEST] Skill Testing Report
            
            [OK] Skill testing and validation completed!
            
            ### Summary
            
            The following tests were executed:
            - [OK] Meta Skills Validation
            - [OK] Skill Functionality Testing  
            - [OK] Performance Analysis
            - [OK] Security Analysis
            
            ### Detailed Results
            
            ${reportContent.split('\n').slice(0, 30).join('\n')}
            
            [View full test report and artifacts in workflow runs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ---
            *This comment was automatically generated by the skill testing workflow.*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });